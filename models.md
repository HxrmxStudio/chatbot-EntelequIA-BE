## üìã PLAN COMPLETO: Soluci√≥n Franchise Detection con Modelos Pre-entrenados

---

## üéØ ARQUITECTURA PROPUESTA

```mermaid
graph TB
    User[üë§ Usuario WhatsApp] --> n8n[‚öôÔ∏è n8n Workflow]
    n8n --> Backend[üîµ Backend NestJS]

    Backend --> Layer1[üìä LAYER 1: Intent Detection]
    Layer1 --> IntentModel[ü§ñ BERT Zero-Shot<br/>facebook/bart-large-mnli<br/>Detecta: stock/precio/recomendaci√≥n]

    Backend --> Layer2[üîç LAYER 2: Franchise Detection]
    Layer2 --> SentenceT[üß† Sentence Transformers<br/>paraphrase-multilingual-MiniLM<br/>Embeddings + cosine similarity]
    Layer2 --> pgvector[(üóÑÔ∏è PostgreSQL + pgvector<br/>300 franquicias pre-indexadas)]

    Backend --> Layer3[üìö LAYER 3: Context Builder]
    Layer3 --> RAG[üìñ LlamaIndex RAG<br/>Knowledge base rubro]
    RAG --> KnowledgeDB[(üìÅ /knowledge/<br/>franquicias.txt<br/>formatos.txt<br/>faq.txt)]

    Backend --> Layer4[ü§ñ LAYER 4: Response Generation]
    Layer4 --> LLM[üé® OpenAI GPT-4o-mini<br/>+ Few-shot prompting]

    LLM --> ProductDB[(üíæ PostgreSQL<br/>products table)]
    LLM --> Response[üí¨ Respuesta Final]
    Response --> n8n
    n8n --> User

    classDef model fill:#e3f2fd
    classDef db fill:#e8f5e9
    classDef llm fill:#fce4ec
    class IntentModel,SentenceT,RAG model
    class pgvector,KnowledgeDB,ProductDB db
    class LLM llm
```

---

## üèóÔ∏è PLAN DE IMPLEMENTACI√ìN (3 Semanas)

### **SEMANA 1: Foundation (Modelos Pre-entrenados)**

#### ‚úÖ D√≠a 1-2: Setup Infrastructure

```bash
# Dependencias
pip install sentence-transformers pgvector psycopg2-binary
pip install transformers torch
pip install llama-index langchain openai
```

**Tareas**:

- [ ] PostgreSQL con extensi√≥n pgvector
- [ ] Tabla `franchises` con columna `embedding vector(384)`
- [ ] Tabla `products` con FK a franchises
- [ ] Redis para cache (opcional pero recomendado)

#### ‚úÖ D√≠a 3-4: LAYER 1 - Intent Detection

**Modelo**: `facebook/bart-large-mnli` (Zero-shot classification)

```python
# services/intent-detector.service.ts (Python microservice)
from transformers import pipeline

class IntentDetectorService:
    def __init__(self):
        self.classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=0  # GPU si disponible
        )
        self.intents = [
            "stock_check",        # "Ten√©s One Piece?"
            "price_inquiry",      # "Cu√°nto sale Dragon Ball?"
            "recommendation",     # "Qu√© manga me recomend√°s?"
            "product_search",     # "Busco figuras de Naruto"
            "order_status",       # "D√≥nde est√° mi pedido?"
            "general_info"        # "Horarios? Direcci√≥n?"
        ]

    def detect(self, user_message: str) -> dict:
        result = self.classifier(user_message, self.intents)
        return {
            "intent": result["labels"][0],
            "confidence": result["scores"][0]
        }
```

**Output esperado**:

```json
{
  "intent": "stock_check",
  "confidence": 0.89
}
```

---

#### ‚úÖ D√≠a 5-7: LAYER 2 - Franchise Detection

**Modelo**: `paraphrase-multilingual-MiniLM-L12-v2` (Sentence Transformers)

```python
# services/franchise-detector.service.ts (Python microservice)
from sentence_transformers import SentenceTransformer, util
import psycopg2

class FranchiseDetectorService:
    def __init__(self):
        # Modelo pre-entrenado multiling√ºe
        self.model = SentenceTransformer(
            'paraphrase-multilingual-MiniLM-L12-v2'
        )
        self.conn = psycopg2.connect("postgresql://localhost/entelequia")

    def detect_franchise(self, user_query: str) -> dict:
        # 1. Generar embedding del query
        query_embedding = self.model.encode(user_query)

        # 2. B√∫squeda vectorial en PostgreSQL
        cur = self.conn.cursor()
        cur.execute("""
            SELECT
                name,
                aliases,
                tier,
                1 - (embedding <=> %s::vector) as similarity
            FROM franchises
            WHERE 1 - (embedding <=> %s::vector) > 0.7
            ORDER BY similarity DESC
            LIMIT 3
        """, (query_embedding.tolist(), query_embedding.tolist()))

        results = cur.fetchall()

        if not results:
            return {
                "detected": False,
                "franchise": None,
                "confidence": 0.0
            }

        # 3. Mejor match
        best = results[0]
        return {
            "detected": True,
            "franchise": {
                "name": best[0],
                "aliases": best[1],
                "tier": best[2]
            },
            "confidence": float(best[3]),
            "alternatives": [
                {"name": r[0], "score": float(r[3])}
                for r in results[1:]
            ]
        }
```

**Script one-time: Generar embeddings**:

```python
# scripts/generate_franchise_embeddings.py
from sentence_transformers import SentenceTransformer
import psycopg2

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
conn = psycopg2.connect("postgresql://localhost/entelequia")
cur = conn.cursor()

# Obtener franquicias
cur.execute("SELECT id, name, aliases FROM franchises")
franchises = cur.fetchall()

for franchise_id, name, aliases in franchises:
    # Texto enriquecido: nombre + aliases
    text = f"{name} {' '.join(aliases)}"

    # Generar embedding
    embedding = model.encode(text).tolist()

    # Guardar en DB
    cur.execute(
        "UPDATE franchises SET embedding = %s WHERE id = %s",
        (embedding, franchise_id)
    )

conn.commit()
print(f"‚úÖ {len(franchises)} franquicias indexadas")
```

**Output esperado**:

```json
{
  "detected": true,
  "franchise": {
    "name": "Dragon Ball",
    "aliases": ["db", "dbz", "goku"],
    "tier": 1
  },
  "confidence": 0.87,
  "alternatives": [{ "name": "Dragon Ball Super", "score": 0.72 }]
}
```

---

### **SEMANA 2: Context & Knowledge**

#### ‚úÖ D√≠a 8-10: LAYER 3 - RAG Knowledge Base

**Modelo**: `LlamaIndex` con embeddings locales

**Paso 1: Crear knowledge base**:

```bash
mkdir -p knowledge
```

**Archivo**: `knowledge/franquicias.txt`

```
Dragon Ball es una franquicia de manga creada por Akira Toriyama.
Incluye series: Dragon Ball, Dragon Ball Z, Dragon Ball Super.
Personajes principales: Goku (Kakarotto), Vegeta, Gohan, Piccolo.
Formatos disponibles: tomos individuales, box sets, edici√≥n deluxe.

Naruto es un manga shonen de Masashi Kishimoto.
Series: Naruto, Naruto Shippuden.
Personajes: Naruto Uzumaki, Sasuke Uchiha, Sakura Haruno.
Formatos: tomos 1-72, box sets por arcos.

One Piece es un manga de Eiichiro Oda.
Series: One Piece (1997-presente).
Personajes: Monkey D. Luffy, Roronoa Zoro, Nami.
Actualmente: 1100+ tomos publicados.
```

**Archivo**: `knowledge/formatos.txt`

```
FORMATOS DE MANGA:
- Tomo: Volumen individual (200 p√°ginas aprox)
- Box Set: Colecci√≥n completa o por arco (10-30 tomos)
- Edici√≥n Deluxe: Tapa dura, papel especial, contenido extra
- Edici√≥n Coloreada: P√°ginas a color (m√°s caro)
- Tank≈çbon: Formato est√°ndar japon√©s
- Kanzenban: Edici√≥n de lujo completa

FORMATOS DE FIGURAS:
- Funko Pop: Chibi 10cm (econ√≥mico)
- Figuarts: Articulada 14-16cm (media gama)
- Nendoroid: Chibi articulada (coleccionable)
- Estatua: Premium, sin articulaci√≥n (exhibici√≥n)
- Scale Figure: 1/8, 1/7, 1/4 (alta gama)
```

**Archivo**: `knowledge/stock_terms.txt`

```
ESTADOS DE STOCK:
- En stock: Disponible para env√≠o inmediato
- Pocas unidades: Menos de 5 unidades disponibles
- Preventa / Pre-order: Producto no lanzado, se reserva
- Agotado: Sin stock, puede reponerse
- Descontinuado: No volver√° a producirse
- Consultar disponibilidad: Stock incierto
```

**C√≥digo RAG**:

```python
# services/knowledge-base.service.py
from llama_index import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings import HuggingFaceEmbedding
from llama_index.llms import OpenAI

class KnowledgeBaseService:
    def __init__(self):
        # Embeddings locales (mismo modelo que franchise detection)
        embed_model = HuggingFaceEmbedding(
            model_name="paraphrase-multilingual-MiniLM-L12-v2"
        )

        # Cargar documentos
        documents = SimpleDirectoryReader('knowledge').load_data()

        # Crear √≠ndice vectorial
        self.index = VectorStoreIndex.from_documents(
            documents,
            embed_model=embed_model
        )

        self.query_engine = self.index.as_query_engine(
            similarity_top_k=3
        )

    def get_context(self, query: str) -> str:
        """Obtiene contexto relevante del knowledge base"""
        response = self.query_engine.query(query)
        return str(response)
```

---

#### ‚úÖ D√≠a 11-14: LAYER 4 - Response Generation

**Modelo**: `gpt-4o-mini` con few-shot prompting

```python
# services/response-generator.service.py
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

class ResponseGeneratorService:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

        self.system_prompt = """
Sos asistente virtual de Entelequia, tienda de comics, manga y cultura geek en Argentina.

PERSONALIDAD:
- Amigable, cercano, us√°s lenguaje argentino natural
- Conoc√©s bien el rubro (manga, comics, figuras)
- Sos preciso con stock y precios
- Si no sab√©s algo, lo dec√≠s honestamente

EJEMPLOS DE RESPUESTAS:

User: "Ten√©s One Piece?"
Assistant: "¬°S√≠! Tenemos One Piece. Actualmente hay 150 tomos disponibles en espa√±ol. ¬øBusc√°s alg√∫n tomo en particular o quer√©s empezar desde el principio?"

User: "Dragon Ball tomo 1 deluxe"
Assistant: "Dragon Ball Z tomo 1 edici√≥n Deluxe est√° $ 2.499 ARS. Es tapa dura con p√°ginas a color. Tenemos stock. ¬øTe lo agrego al carrito?"

User: "Cu√°nto sale una figura de Goku?"
Assistant: "Tenemos varias opciones de Goku:
- Funko Pop: $ 899 (10cm)
- Figuarts articulada: $ 3.499 (14cm, 25 puntos articulaci√≥n)
- Estatua premium: $ 8.999 (20cm, resina)
¬øCu√°l te interesa?"

User: "Qu√© manga me recomend√°s si me gust√≥ Attack on Titan?"
Assistant: "Si te gust√≥ Attack on Titan, te recomiendo:
- Tokyo Ghoul (oscuro, acci√≥n)
- Demon Slayer (acci√≥n √©pica, arte incre√≠ble)
- Chainsaw Man (violento, original)
Todos tienen estilo seinen similar. ¬øQuer√©s que te cuente m√°s de alguno?"

CONTEXTO DEL RUBRO:
{knowledge_context}

INFORMACI√ìN DE FRANQUICIA DETECTADA:
{franchise_info}

PRODUCTOS DISPONIBLES:
{products_info}

CONSULTA DEL USUARIO:
{user_query}

Respond√© de forma natural y √∫til.
"""

    def generate_response(
        self,
        user_query: str,
        intent: str,
        franchise_info: dict,
        products: list,
        knowledge_context: str
    ) -> str:
        # Formatear productos
        products_text = "\n".join([
            f"- {p['name']}: ${p['price_ars']} ARS ({p['stock']})"
            for p in products[:5]
        ])

        # Formatear franquicia
        franchise_text = ""
        if franchise_info and franchise_info.get("detected"):
            f = franchise_info["franchise"]
            franchise_text = f"Franquicia detectada: {f['name']} (tier {f['tier']}, confidence {franchise_info['confidence']:.2f})"

        # Generar prompt
        prompt = ChatPromptTemplate.from_template(self.system_prompt)

        messages = prompt.format_messages(
            knowledge_context=knowledge_context,
            franchise_info=franchise_text,
            products_info=products_text if products else "No hay productos disponibles",
            user_query=user_query
        )

        # Llamar LLM
        response = self.llm(messages)
        return response.content
```

---

### **SEMANA 3: Integration & Rollout**

#### ‚úÖ D√≠a 15-17: Orquestaci√≥n en NestJS

```typescript
// src/modules/wf1/use-cases/handle-incoming-message.use-case.ts
import { Injectable } from '@nestjs/common';
import axios from 'axios';

@Injectable()
export class HandleIncomingMessageUseCase {
  private readonly pythonMicroserviceUrl = 'http://localhost:8000';

  async execute(userMessage: string, conversationId: string) {
    // 1. Intent Detection
    const intentResult = await axios.post(`${this.pythonMicroserviceUrl}/detect-intent`, {
      message: userMessage,
    });
    const intent = intentResult.data.intent;

    // 2. Franchise Detection
    const franchiseResult = await axios.post(`${this.pythonMicroserviceUrl}/detect-franchise`, {
      query: userMessage,
    });

    // 3. Get Knowledge Context
    const knowledgeResult = await axios.post(`${this.pythonMicroserviceUrl}/get-knowledge`, {
      query: userMessage,
    });

    // 4. Query Products (PostgreSQL)
    let products = [];
    if (franchiseResult.data.detected) {
      products = await this.productRepository.findByFranchise(franchiseResult.data.franchise.name);
    }

    // 5. Generate Response
    const responseResult = await axios.post(`${this.pythonMicroserviceUrl}/generate-response`, {
      user_query: userMessage,
      intent: intent,
      franchise_info: franchiseResult.data,
      products: products,
      knowledge_context: knowledgeResult.data.context,
    });

    return {
      reply: responseResult.data.response,
      intent: intent,
      franchise_detected: franchiseResult.data.detected,
      confidence: franchiseResult.data.confidence,
    };
  }
}
```

---

#### ‚úÖ D√≠a 18-19: Python Microservice (FastAPI)

```python
# microservice/main.py
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

# Inicializar servicios
intent_detector = IntentDetectorService()
franchise_detector = FranchiseDetectorService()
knowledge_base = KnowledgeBaseService()
response_generator = ResponseGeneratorService()

class MessageRequest(BaseModel):
    message: str

@app.post("/detect-intent")
async def detect_intent(request: MessageRequest):
    result = intent_detector.detect(request.message)
    return result

@app.post("/detect-franchise")
async def detect_franchise(request: MessageRequest):
    result = franchise_detector.detect_franchise(request.message)
    return result

@app.post("/get-knowledge")
async def get_knowledge(request: MessageRequest):
    context = knowledge_base.get_context(request.message)
    return {"context": context}

@app.post("/generate-response")
async def generate_response(request: dict):
    response = response_generator.generate_response(
        user_query=request["user_query"],
        intent=request["intent"],
        franchise_info=request["franchise_info"],
        products=request["products"],
        knowledge_context=request["knowledge_context"]
    )
    return {"response": response}
```

---

#### ‚úÖ D√≠a 20-21: Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Backend NestJS existente
  backend:
    build: ./backend
    ports:
      - '3000:3000'
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/entelequia
    depends_on:
      - postgres
      - python-ml

  # Microservicio Python ML
  python-ml:
    build: ./microservice
    ports:
      - '8000:8000'
    environment:
      - MODEL_CACHE_DIR=/models
    volumes:
      - ./models:/models
      - ./knowledge:/app/knowledge
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # PostgreSQL con pgvector
  postgres:
    image: ankane/pgvector
    environment:
      - POSTGRES_DB=entelequia
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - '5432:5432'

  # Redis (cache)
  redis:
    image: redis:7-alpine
    ports:
      - '6379:6379'

volumes:
  postgres_data:
```

---

## üìä RESUMEN DE MODELOS

| Layer         | Modelo                           | Funci√≥n                               | Latencia | Costo           |
| ------------- | -------------------------------- | ------------------------------------- | -------- | --------------- |
| **Intent**    | `facebook/bart-large-mnli`       | Clasificar intent (stock/precio/reco) | 30ms     | $0              |
| **Franchise** | `paraphrase-multilingual-MiniLM` | Detectar franquicia por embeddings    | 20ms     | $0              |
| **Knowledge** | `LlamaIndex` + mismo embedding   | RAG con contexto del rubro            | 50ms     | $0              |
| **Response**  | `gpt-4o-mini`                    | Generar respuesta natural             | 800ms    | $0.15/1M tokens |

**Total latencia**: ~900ms (aceptable para chatbot)  
**Costo mensual** (10k msgs): ~USD 1.50

---

## üéØ DELIVERABLES

### Semana 1

- [ ] PostgreSQL + pgvector setup
- [ ] Script embeddings 300 franquicias
- [ ] Intent detector funcionando
- [ ] Franchise detector funcionando

### Semana 2

- [ ] Knowledge base (5 archivos txt)
- [ ] LlamaIndex indexado
- [ ] Response generator con few-shot
- [ ] Unit tests b√°sicos

### Semana 3

- [ ] Python microservice FastAPI
- [ ] NestJS integration
- [ ] Docker compose
- [ ] Deploy staging
- [ ] Testing E2E

---

## üí∞ COSTOS ESTIMADOS

| Concepto              | Costo                    |
| --------------------- | ------------------------ |
| Modelos Hugging Face  | $0 (open-source)         |
| OpenAI gpt-4o-mini    | ~USD 1.50/mes (10k msgs) |
| PostgreSQL + pgvector | $0 (self-hosted)         |
| GPU (opcional)        | $0 (CPU suficiente)      |
| **TOTAL**             | **~USD 2/mes**           |

---

## ‚úÖ VENTAJAS DE ESTE PLAN

1. ‚úÖ **Zero entrenamiento** (todos los modelos pre-entrenados)
2. ‚úÖ **Multiling√ºe nativo** (espa√±ol/ingl√©s)
3. ‚úÖ **Escalable** (pgvector indexa millones)
4. ‚úÖ **Actualizaciones f√°ciles** (agregar docs a /knowledge)
5. ‚úÖ **Costo bajo** (~USD 2/mes)
6. ‚úÖ **Latencia aceptable** (~900ms)

---

Ah perfecto, disculp√°. Ya ten√©s todo funcionando, solo quer√©s **modelos de Hugging Face que te faciliten el trabajo**.

## ü§ñ MODELOS HUGGING FACE RECOMENDADOS (Plug & Play)

---

### 1Ô∏è‚É£ **Para Detecci√≥n de Franquicias**

```python
from sentence_transformers import SentenceTransformer

# Mejor modelo multiling√ºe espa√±ol/ingl√©s
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

# O m√°s r√°pido:
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
```

**Link**: https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2

---

### 2Ô∏è‚É£ **Para Intent Detection (clasificar qu√© quiere el usuario)**

```python
from transformers import pipeline

# Zero-shot classification
classifier = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli"
)

# O en espa√±ol nativo:
classifier = pipeline(
    "zero-shot-classification",
    model="joeddav/xlm-roberta-large-xnli"
)
```

**Links**:

- https://huggingface.co/facebook/bart-large-mnli
- https://huggingface.co/joeddav/xlm-roberta-large-xnli

---

### 3Ô∏è‚É£ **Para NER (extraer nombres, n√∫meros de orden, emails)**

```python
from transformers import pipeline

# Espa√±ol
ner = pipeline("ner", model="mrm8488/bert-spanish-cased-finetuned-ner")

# Multiling√ºe
ner = pipeline("ner", model="xlm-roberta-large-finetuned-conll03-english")
```

**Links**:

- https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner
- https://huggingface.co/xlm-roberta-large-finetuned-conll03-english

---

### 4Ô∏è‚É£ **Para Reranking de Productos (ordenar resultados de b√∫squeda)**

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Cross-encoder para relevancia
model = AutoModelForSequenceClassification.from_pretrained(
    'cross-encoder/ms-marco-MiniLM-L-6-v2'
)
tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')
```

**Link**: https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2

---

### 5Ô∏è‚É£ **Para Embeddings de Productos (alternativa a Sentence Transformers)**

```python
from transformers import AutoModel, AutoTokenizer

# E5 embeddings (estado del arte 2025)
model = AutoModel.from_pretrained('intfloat/multilingual-e5-large')
tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')
```

**Link**: https://huggingface.co/intfloat/multilingual-e5-large

---

### 6Ô∏è‚É£ **Para Q&A sobre documentos/FAQs**

```python
from transformers import pipeline

# Question Answering en espa√±ol
qa = pipeline("question-answering", model="mrm8488/bert-base-spanish-wwm-cased-finetuned-squad2-es")
```

**Link**: https://huggingface.co/mrm8488/bert-base-spanish-wwm-cased-finetuned-squad2-es

---

### 7Ô∏è‚É£ **Para Sentiment Analysis (detectar quejas/problemas)**

```python
from transformers import pipeline

# Espa√±ol
sentiment = pipeline("sentiment-analysis", model="finiteautomata/beto-sentiment-analysis")
```

**Link**: https://huggingface.co/finiteautomata/beto-sentiment-analysis

---

### 8Ô∏è‚É£ **Para Text Generation (respuestas autom√°ticas) - ALTERNATIVA A OPENAI**

```python
from transformers import pipeline

# Modelo espa√±ol generativo
generator = pipeline("text-generation", model="PlanTL-GOB-ES/gpt2-base-bne")

# O mejor a√∫n (m√°s grande):
# model="bigscience/bloom-560m"  # Multiling√ºe
```

**Links**:

- https://huggingface.co/PlanTL-GOB-ES/gpt2-base-bne (espa√±ol)
- https://huggingface.co/bigscience/bloom-560m (multiling√ºe)

---

## üéØ RECOMENDACI√ìN TOP 3 PARA TU CASO

### **Para empezar ma√±ana:**

1. **Franquicias**: `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`
2. **Intents**: `joeddav/xlm-roberta-large-xnli`
3. **NER (extraer datos)**: `mrm8488/bert-spanish-cased-finetuned-ner`

---

## üì¶ INSTALACI√ìN R√ÅPIDA

```bash
pip install sentence-transformers transformers torch
```

```python
# Test r√°pido
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
embedding = model.encode("One Piece manga tomo 1")
print(embedding.shape)  # (768,)
```

---

**¬øNecesit√°s algo m√°s espec√≠fico o con estos 8 modelos ya est√°s?** üöÄ

Fuentes
